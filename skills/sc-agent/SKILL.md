# SC Agent - Session Orchestrator

**Purpose:** Coordinates task workflow from clarification through implementation to validation, delegating to specialized skills for maximum efficiency.

**When to invoke:** Use this skill when you need structured guidance on how to approach a complex multi-step task with proper investigation, confidence checking, and validation.

---

## What This Skill Does

SC Agent provides a systematic 5-step workflow for tackling tasks:

1. **Clarify scope** - Define success criteria and constraints
2. **Plan investigation** - Use parallel execution and delegate to specialized skills
3. **Iterate until confident** - Track confidence, require â‰¥0.90 before implementation
4. **Implementation wave** - Execute with TDD, complexity assessment, planning (medium/hard), loop detection (max 20 attempts), activity logging to engineering_log.md, and educational explanations
5. **Self-review and reflexion** - Validate outcomes and identify follow-up

## When to Use

Use sc-agent for:
- Complex tasks requiring investigation before implementation
- Multi-step workflows that benefit from orchestration
- Tasks where confidence checking prevents wasted effort
- Work requiring coordination between research, implementation, and validation

**Do NOT use for:**
- Simple, well-understood single-step tasks
- Urgent fixes where the solution is obvious
- Exploratory/research-only tasks

## Engineering Log

SC Agent maintains **engineering_log.md** in the workspace root to track implementation activity:

**Location:** `<workspace-root>/engineering_log.md`

**Purpose:** Succinct activity log showing:
- Task start/completion timestamps
- Complexity assessments
- Implementation attempts and progress
- Key technical decisions
- Blockers and resolutions
- Duration/effort tracking

**Format:** Markdown with timestamped sections (see Implementation Wave â†’ Activity Logging for details)

**Benefit:** Historical record of development decisions and patterns for future reference

---

## 5-Step Task Protocol

### 1. Clarify Scope
- Confirm success criteria, blockers, and constraints
- Capture acceptance tests that matter
- Ask clarifying questions before proceeding

### 2. Plan Investigation
- Use **parallel tool calls** where possible
- Delegate to specialized skills:
  - **@confidence-check** - Pre-implementation assessment (â‰¥0.90 required)
  - **@deep-research** - Web/MCP research for unknowns
  - **@repo-index** - Repository structure and file discovery
  - **@self-review** - Post-implementation validation
- Prefer existing helpers over inventing new approaches

### 3. Iterate Until Confident
- Track confidence score from skill results
- **Do NOT implement below 0.90 confidence**
- Escalate to user if confidence stalls or new context needed
- Log confidence changes: `ðŸ“Š Confidence: 0.82 â†’ 0.93`

### 4. Implementation Wave

#### A. Assess Task Complexity
Evaluate before implementing:
- **Simple:** Single file, <50 lines, clear path â†’ Skip to TDD
- **Medium:** Multiple files, integration needed, some uncertainty â†’ Plan first
- **Hard:** Architecture changes, multiple systems, high uncertainty â†’ Deep planning

**Educational note:** Explain complexity assessment to user with reasoning.

#### B. Planning Phase (Medium/Hard tasks only)
Before writing any code:
1. Break down approach into 3-7 concrete steps
2. Identify integration points and dependencies
3. List potential blockers or unknowns
4. Present plan to user: "Here's my approach: [bullets]. Does this make sense?"
5. Wait for user confirmation before proceeding

**Educational note:** Explain WHY you chose this approach over alternatives.

#### C. Test-Driven Development (ALL tasks)
ALWAYS write tests first:

1. **Write failing tests:**
   - Create or update test file
   - Write test cases for success path
   - Write test cases for edge cases
   - Run tests (expect failures)
   - **Explain:** "These tests define what success looks like for [feature]"

2. **Implement to pass tests:**
   - Write minimal implementation to pass tests
   - Run tests after each significant change
   - **Explain:** "This implementation handles [scenario] by [approach]"
   - Track attempts (see Loop Detection below)

3. **Refactor:**
   - Clean up code while keeping tests green
   - Add inline documentation (explain WHY, not what)
   - **Explain:** "Refactoring to improve [quality aspect]"

#### D. Loop Detection
Track implementation attempts to prevent infinite loops:
- **Max attempts:** 20 tries per implementation step
- **Same error threshold:** If identical error 3 times in a row â†’ Stop and reassess
- **No progress threshold:** If no tests passing after 10 attempts â†’ Stop and report

**When stuck:**
1. Log to engineering_log.md (see Logging below)
2. Report to user:
   ```
   âš ï¸ Stuck after [N] attempts.
   Error: [last error]
   Tried: [approaches attempted]

   Options:
   1. Research [topic] for better approach
   2. Adjust requirements or acceptance criteria
   3. Break into smaller sub-tasks
   4. Skip and mark as blocked

   What would you like to do?
   ```
3. Wait for user decision (NEVER loop infinitely)

#### E. Activity Logging
Maintain engineering_log.md with DETAILED narrative summaries:

**At task start:**
```markdown
## [YYYY-MM-DD HH:MM] Task: [Task ID/Name]

**Complexity:** [Simple/Medium/Hard]
**Approach:** [1-2 sentence strategy]
**Initial Understanding:** [What you know about the problem]
**Unknowns:** [What you need to figure out]
```

**During implementation - NARRATIVE FORMAT:**
```markdown
### Implementation Narrative

ðŸ”„ **Attempt 1: [What you tried]**
- **Why this approach:** [Reasoning for choosing this method]
- **What I did:** [Step-by-step actions taken]
- **Result:** [âœ… Worked / âŒ Failed / âš ï¸ Partial]
- **What I learned:** [Key insight from this attempt]

ðŸ”„ **Attempt 2: [What you tried next]** (if applicable)
- **Why I switched:** [Root cause of previous failure, why you're trying something new]
- **Alternatives considered:** [What else you thought about and why you rejected it]
- **What I did:** [Step-by-step actions taken]
- **Result:** [âœ… Worked / âŒ Failed / âš ï¸ Partial]
- **What I learned:** [Key insight from this attempt]

[Continue for each significant attempt or pivot]

### Tests Written
- âœ… [count] test cases covering: [what aspects]
- ðŸ”„ Tests passing: [count]/[total]
- âš ï¸ Failed tests: [what failed and why, if any]
```

**At completion or blockage:**
```markdown
### Result: [Complete/Blocked/Partial]

**Files changed:** [list with brief description of what changed in each]
**Tests:** [passing]/[total]

**Problem-Solving Journey Summary:**
1. [Started with X approach because Y]
2. [Hit problem Z, diagnosed it as...]
3. [Tried alternative W, which worked because...]
4. [Final solution uses Q for these reasons...]

**Key Decisions & Trade-offs:**
- [Decision 1]: Chose X over Y because [reasoning]
- [Decision 2]: Accepted trade-off of [downside] to gain [benefit]
- [Decision 3]: [etc.]

**Alternatives Considered & Rejected:**
- [Option A]: Rejected because [specific reason]
- [Option B]: Rejected because [specific reason]

**Blockers:** [if any - with detailed explanation]
**Duration:** [approximate time/attempts]
**Confidence in solution:** [High/Medium/Low and why]
```

#### F. Educational Explanations
Throughout implementation, explain:
1. **What** you're doing (the action)
2. **Why** you chose this approach (the reasoning)
3. **Alternatives** you considered (the trade-offs)
4. **What to learn** from this (the takeaway)

**Example:**
```
I'm using a factory pattern here because we need to create different
email providers (SendGrid, AWS SES) from configuration. I considered
a singleton but rejected it because we might need multiple instances
for testing. This makes the code more flexible but adds a bit more
complexity. Key takeaway: Factories are great when you need runtime
object creation based on conditions.
```

#### G. Grouped Execution
- Prepare edits as checkpoint summaries
- Run tests after each logical group of changes
- Use parallel tool calls where possible

### 5. Self-Review and Reflexion
- Invoke **@self-review** to validate outcomes
- Share residual risks or follow-up tasks
- Document lessons learned if applicable

---

## Delegation Patterns

SC Agent **delegates** to specialized skills rather than doing everything itself:

| Skill | When to Delegate | Expected Output |
|-------|-----------------|-----------------|
| **@confidence-check** | Before any implementation | Confidence score â‰¥0.90 |
| **@deep-research** | Need external info or docs | Research summary with sources |
| **@repo-index** | First task in session or codebase drift | Repository structure briefing |
| **@self-review** | After implementation complete | Validation results with evidence |

**Example delegation:**
```
User task: "Add user authentication"

SC Agent workflow:
1. Clarify â†’ Ask about auth method, requirements, acceptance criteria
2. Investigate â†’ @confidence-check (check for existing auth, find OSS examples)
3. Research â†’ @deep-research (JWT best practices, security considerations)
4. Implement (with confidence â‰¥0.90):
   a. Assess complexity: Medium (multiple files, integration with existing code)
   b. Plan approach: [show 5-step plan, get user confirmation]
   c. TDD: Write auth tests first (login, logout, token refresh)
   d. Implement: Build to pass tests (attempt 1-20, track progress)
   e. Log to engineering_log.md: Document decisions and progress
   f. Explain: "Using JWT because it's stateless. Considered sessions but
      they require server-side storage. Trade-off: Must handle token expiry."
5. Validate â†’ @self-review (verify tests pass, security checks, requirements met)
```

---

## Tooling Guidance

**Repository Awareness:**
- Call **@repo-index** on first task per session
- Re-invoke when codebase drifts significantly

**Research:**
- Delegate to **@deep-research** for open questions
- Don't speculate when external lookup can provide facts

**Confidence Tracking:**
- Log score whenever it changes
- User can see progress: `ðŸ”„ Investigatingâ€¦ â†’ ðŸ“Š Confidence: 0.85 â†’ âœ… 0.92`

**Fallback:**
- If tool/MCP unavailable, note failure and fall back to native techniques
- Flag gaps for follow-up

---

## Token Discipline

Keep communication **concise and actionable**:

âœ… **Good:**
- `ðŸ”„ Investigating duplicatesâ€¦`
- `ðŸ“Š Confidence: 0.82 (need OSS reference)`
- `âœ… Implementation complete, tests passing`

âŒ **Avoid:**
- Re-explaining background facts already established
- Long redundant summaries
- Repeating context from earlier in session

**Archive long briefs** in memory tools only if user requests persistence.

---

## Output Format

When using SC Agent, provide **concise updates at end of each phase**:

```
## Phase: [Clarification/Investigation/Implementation/Review]

**Status:** [In Progress/Complete/Blocked]
**Confidence:** [0.XX] (if applicable)
**Attempt:** [N/20] (during implementation if relevant)

**Key Actions:**
- [Action taken]
- [Delegation made]

**Educational Note:** [Brief explanation of what/why/alternatives]

**Next Steps:**
- [What happens next]

**Blockers:** [Any issues or questions]

**Logged to:** engineering_log.md (during implementation)
```

**Example:**
```
## Phase: Investigation

**Status:** Complete
**Confidence:** 0.93

**Key Actions:**
- âœ… Checked for duplicates (Grep/Glob) - none found
- âœ… Verified official docs (WebFetch)
- âœ… Found OSS reference (GitHub)

**Next Steps:**
- Proceeding to implementation (confidence â‰¥0.90)

**Blockers:** None
```

**Example (Implementation Phase):**
```
## Phase: Implementation

**Status:** In Progress
**Complexity:** Medium
**Attempt:** 3/20

**Key Actions:**
- âœ… Tests written: 5 test cases
- ðŸ”„ Implementing EmailService class
- âœ… Tests passing: 3/5

**Educational Note:**
Using dependency injection for EmailProvider to make testing easier.
Considered hardcoding SendGrid but that would make it impossible to swap
providers later or mock in tests. Trade-off: Slightly more setup code
but much better testability.

**Next Steps:**
- Fix remaining 2 test failures
- Add error handling for API failures

**Blockers:** None

**Logged to:** engineering_log.md
```

---

## Success Criteria

You'll know SC Agent is working when:
1. âœ… Tasks follow 5-step protocol systematically
2. âœ… Specialized skills are delegated to (not reinvented)
3. âœ… Confidence â‰¥0.90 before implementation
4. âœ… Tests written BEFORE implementation (TDD)
5. âœ… Complexity assessed and planning done for medium/hard tasks
6. âœ… Loop detection active (max 20 attempts, stops if stuck)
7. âœ… Activity logged to engineering_log.md with succinct summaries
8. âœ… Educational explanations provided (what/why/alternatives/takeaway)
9. âœ… Updates are concise and progress-focused
10. âœ… Self-review validates outcomes with evidence

---

## Integration with Other Skills

SC Agent **orchestrates** the workflow by invoking:
- **confidence-check** (pre-implementation)
- **deep-research** (investigation)
- **repo-index** (repository awareness)
- **self-review** (post-implementation)

This creates a **complete workflow** from task assignment to validated delivery.
